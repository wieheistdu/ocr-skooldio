{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Detection with Surya\n",
    "\n",
    "This notebook will demonstrate how to use `Surya` to detect text in an image. Surya is named for the Hindu sun god, who has universal vision. Find out more about Surya [here](https://github.com/VikParuchuri/surya).\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, we need to install the `surya-ocr` library. You can install it using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Surya\n",
    "%%capture\n",
    "!pip install surya-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Surya Detector and Recognizer\n",
    "\n",
    "We will first initialize the detector and recognizer. The detector is used to detect text in an image, and the recognizer is used to recognize the text in the detected regions. For the first time, the library will download the pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surya.model.recognition.model import load_model as load_recognizer\n",
    "from surya.model.recognition.processor import (load_processor as load_recognizer_processor,)\n",
    "\n",
    "recognizer = load_recognizer()\n",
    "recognizer_processor = load_recognizer_processor()\n",
    "\n",
    "from surya.model.detection.model import load_model as load_detector\n",
    "from surya.model.detection.model import load_processor as load_detector_processor\n",
    "\n",
    "detector = load_detector()\n",
    "detector_processor = load_detector_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) We can compile the decoder of the recognizer to speed up the recognition process. This is done by calling the `torch.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "recognizer.decoder.model = torch.compile(recognizer.decoder.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Display the Image\n",
    "\n",
    "We will load the our document image and display it using jupyter notebook built-in `display` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load the image from a path\n",
    "image_path = \"path/to/image.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Text Detection and Recognition\n",
    "\n",
    "We will use the `run_ocr` method to detect and recognize text in the image. This method returns a list of `OCRResult`, where `OCRResult` is the result of the individual document image. This is the structure of `OCRResult`:\n",
    "\n",
    "```py\n",
    "# Document level result\n",
    "class OCRResult(BaseModel):\n",
    "    text_lines: List[TextLine]\n",
    "    languages: List[str]\n",
    "    image_bbox: List[float]\n",
    "```\n",
    "\n",
    "which we will use only the `text_lines` attribute. Here is the structure of `TextLine`:\n",
    "\n",
    "```py\n",
    "# Text line level result\n",
    "class TextLine:\n",
    "    text: str                   # Detected text\n",
    "    confidence: float           # 0.0 to 1.0\n",
    "    polygon: List[List[float]]  # [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surya.ocr import run_ocr\n",
    "from surya.schema import OCRResult, TextLine\n",
    "\n",
    "# Define languages to recognize\n",
    "langs = [\"en\", \"th\"]\n",
    "predictions: list[OCRResult] = run_ocr([image], [langs], detector, detector_processor, recognizer, recognizer_processor)\n",
    "\n",
    "# Because we only have one image, our result will be in the first element\n",
    "prediction = predictions[0]\n",
    "# Unpack the prediction\n",
    "text_lines: list[TextLine] = prediction.text_lines\n",
    "\n",
    "# Let's print the first 5 textlines\n",
    "for idx, text_line in enumerate(text_lines):\n",
    "    if idx == 5:\n",
    "        break\n",
    "\n",
    "    print(f\"Textline {idx}: {text_line.text}\")\n",
    "    print(f\"Confidence: {text_line.confidence}\")\n",
    "    print(f\"Bounding box: {text_line.polygon}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Bounding Boxes and Display the Image with Predictions\n",
    "\n",
    "We will draw the bounding boxes around the detected text and display the image with the text predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont\n",
    "from surya.schema import OCRResult\n",
    "\n",
    "FONT = ImageFont.truetype(\"../assets/THSarabun.ttf\", size=20)\n",
    "\n",
    "\n",
    "def draw_boxes(image: Image.Image, result: OCRResult) -> None:\n",
    "    \"\"\"Draw bounding boxes and its information on the image.\"\"\"\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Draw each result on the image.\n",
    "    for text_line in result.text_lines:\n",
    "        # Unpack the result.\n",
    "        text = text_line.text\n",
    "        confidence_score = text_line.confidence\n",
    "        bbox = text_line.polygon\n",
    "\n",
    "        # bbox is a four-point coordinate of the bounding box. [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        # we need to convert it to PIL coordinates to draw the rectangle.\n",
    "        # which is only two points, top-left and bottom-right. [x1, y1, x2, y2]\n",
    "\n",
    "        pil_bbox = [bbox[0][0], bbox[0][1], bbox[2][0], bbox[2][1]]\n",
    "\n",
    "        # 1. Draw the bounding box\n",
    "        draw.rectangle(pil_bbox, outline=\"blue\", width=2)\n",
    "        # 2. Draw the text and confidence score e.g. 'Hello (0.72)'\n",
    "        draw_text = f\"{text} ({confidence_score:.2f})\"\n",
    "\n",
    "        # Place text at the top-left of the bounding box and shift it up by 20 pixels\n",
    "        x = pil_bbox[0]\n",
    "        y = pil_bbox[1] - 20\n",
    "        draw.text((x, y), draw_text, fill=\"red\", font=FONT)\n",
    "\n",
    "\n",
    "# Create a copy of the image to draw on\n",
    "image_with_boxes = image.copy()\n",
    "draw_boxes(image_with_boxes, prediction)\n",
    "\n",
    "display(image_with_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use only the `surya` detector and `surya` recognizer to perform text detection and recognition separately. This is useful when we want to use our own text detection model or our own text recognition model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Detection with Surya\n",
    " This method returns a list of `TextDetectionResult`, where `TextDetectionResult` is the result of the individual document image. This is the structure of `TextDetectionResult`:\n",
    "```py\n",
    "class TextDetectionResult(BaseModel):\n",
    "    bboxes: List[PolygonBox]\n",
    "    vertical_lines: List[ColumnLine]\n",
    "    heatmap: Any\n",
    "    affinity_map: Any\n",
    "    image_bbox: List[float]\n",
    "```\n",
    "\n",
    "which we will use only the `bboxes` attribute. Here is the structure of `PolygonBox`:\n",
    "\n",
    "```py\n",
    "class PolygonBox(BaseModel):\n",
    "    polygon: List[List[float]]\n",
    "    confidence: Optional[float] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surya.detection import batch_text_detection\n",
    "from surya.schema import TextDetectionResult\n",
    "\n",
    "bbox_predictions: list[TextDetectionResult] = batch_text_detection([image], detector, detector_processor)\n",
    "\n",
    "# Because we only have one image, our result will be in the first element\n",
    "bbox_prediction = bbox_predictions[0]\n",
    "\n",
    "# Unpack the prediction\n",
    "bboxes = bbox_prediction.bboxes\n",
    "\n",
    "# Let's print the first 5 bounding boxes\n",
    "for idx, bbox in enumerate(bboxes):\n",
    "    if idx == 5:\n",
    "        break\n",
    "\n",
    "    print(f\"Bounding box {idx}: {bbox.polygon}\")\n",
    "    print(f\"Confidence: {bbox.confidence}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can draw the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "# Create a copy of the image to draw on\n",
    "image_with_boxes = image.copy()\n",
    "\n",
    "# Create a drawing object\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "# Draw each region on the image\n",
    "for bbox in bboxes:\n",
    "    # Unpack the result.\n",
    "    confidence_score = bbox.confidence\n",
    "    bbox = bbox.polygon\n",
    "\n",
    "    # bbox is a four-point coordinate of the bounding box. [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "    # we need to convert it to PIL coordinates to draw the rectangle.\n",
    "    # which is only two points, top-left and bottom-right. [x1, y1, x2, y2]\n",
    "\n",
    "    pil_bbox = [bbox[0][0], bbox[0][1], bbox[2][0], bbox[2][1]]\n",
    "\n",
    "    # 1. Draw the bounding box\n",
    "    draw.rectangle(pil_bbox, outline=\"blue\", width=2)\n",
    "    # 2. Draw the confidence score e.g. '0.72'\n",
    "    draw_text = f\"{confidence_score:.2f}\"\n",
    "    # Shift the text to the left of the bounding box to make it more visible\n",
    "    draw.text((pil_bbox[0], pil_bbox[1]-20), draw_text, fill=\"red\", font=FONT)\n",
    "\n",
    "\n",
    "display(image_with_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's crop the detected text regions and perform text recognition using the `surya` recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the textboxes from the image\n",
    "textboxes = []\n",
    "for bbox in bboxes:\n",
    "    bbox = bbox.polygon\n",
    "    # bbox is a four-point coordinate of the bounding box. [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "    # we need to convert it to PIL coordinates to draw the rectangle.\n",
    "    # which is only two points, top-left and bottom-right. [x1, y1, x2, y2]\n",
    "    textboxes.append(image.crop([bbox[0][0], bbox[0][1], bbox[2][0], bbox[2][1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Recognition with Surya\n",
    " This method returns a tuple of `List[str]` and `List[float]`, where the first element is the list of recognized text and the second element is the list of confidence scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surya.recognition import batch_recognition\n",
    "\n",
    "# The number of languages must match the number of textboxes\n",
    "# So we need to pass a list of languages for each textbox\n",
    "languages = [[\"en\", \"th\"]] * len(textboxes)\n",
    "text_predictions, confidence_scores = batch_recognition(textboxes, languages, recognizer, recognizer_processor)\n",
    "\n",
    "# Let's print the first 5 text predictions\n",
    "for idx, (text, confidence_score, textbox) in enumerate(zip(text_predictions, confidence_scores, textboxes)):\n",
    "    if idx == 5:\n",
    "        break\n",
    "\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Confidence: {confidence_score}\")\n",
    "    display(textbox)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Information from the recognized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create extract question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_question = \"\"\"\n",
    "You are provided with a recognized text from the OCR system of a Thai vehicle registration book (สมุดทะเบียนรถ), which each recognized text are seperated by tab (\\t) character.\n",
    "Your task is to extract the following information from the image.\n",
    "The extracted value is typically located on the right side of the key in the document.\n",
    "Some of the text might be corrupted, missing diacritics or misread, autocorrection is appreciated.\n",
    "Extract these details:\n",
    "\n",
    "1. วันจดทะเบียน (date_of_registration)\n",
    "2. เลขทะเบียน (registration_no)\n",
    "3. จังหวัด (car_province)\n",
    "4. ประเภท (vehicle_use)\n",
    "5. รย. (type)\n",
    "6. ลักษณะ (body_style)\n",
    "7. ยี่ห้อรถ (manufacturer)\n",
    "8. แบบ (model)\n",
    "9. รุ่นปี คศ (year)\n",
    "10. สี (color)\n",
    "11. เลขตัวรถ (chassis_number)\n",
    "12. อยู่ที่ (chassis_location)\n",
    "13. ยี่ห้อเครื่องยนต์ (engine_manufacturer)\n",
    "14. เลขเครื่องยนต์ (engine_number)\n",
    "15. อยู่ที่ (engine_location)\n",
    "16. เชื้อเพลิง (fuel_type)\n",
    "17. เลขถังแก๊ส (fuel_tank_number)\n",
    "18. จำนวน (cylinders)\n",
    "19. ซีซี (cubic_capacity)\n",
    "20. แรงม้า (horse_power)\n",
    "21. จำนวนเพลาและล้อ (axles_wheels_no)\n",
    "22. น้ำหนักรถ (unladen_weight)\n",
    "23. น้ำหนักบรรทุก/น้ำหนักเพลา (load_capacity)\n",
    "24. น้ำหนักรวม (gross_weight)\n",
    "25. ที่นั่ง (seats)\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Carefully examine the image and locate each piece of information.\n",
    "If a particular field is not visible or not present in the image, use the value \"N/A\" for that field.\n",
    "Ensure all text extracted from the image is in its original language (Thai or English) as it appears in the document.\n",
    "Return the extracted information in a JSON format, using the English key names provided in parentheses.\n",
    "Only return the JSON output, without any additional explanation or text.\n",
    "\n",
    "Example of expected output in dictionary format:\n",
    "{\n",
    "  \"date_of_registration\": \"1 ม.ค. 2566\",\n",
    "  \"registration_no\": \"กข 1234\",\n",
    "  \"car_province\": \"กรุงเทพมหานคร\",\n",
    "  ...\n",
    "  \"seats\": \"4\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform information extraction with Llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack recognized text into a single string for prompting\n",
    "recognized_text = \"\\t\".join(text_predictions)\n",
    "recognized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\", stop=[\"<|eot_id|>\"]) # Added stop token\n",
    "\n",
    "def get_model_response(user_prompt: str, system_prompt: str) -> str:\n",
    "    # NOTE: No f string and no whitespace in curly braces\n",
    "    template = \"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {system_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        {user_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "\n",
    "    # Added prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"system_prompt\", \"user_prompt\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    # Modified invoking the model\n",
    "    response = llm.invoke(prompt.format(system_prompt=system_prompt, user_prompt=user_prompt))\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "user_prompt = recognized_text\n",
    "system_prompt = extract_question\n",
    "answer = get_model_response(user_prompt, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
